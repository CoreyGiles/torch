% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/ignite.R
\name{optim_ignite_sgd}
\alias{optim_ignite_sgd}
\title{LibTorch implementation of SGD}
\usage{
optim_ignite_sgd(
  params,
  lr = 0.001,
  momentum = 0.9,
  dampening = 0,
  weight_decay = 0.01,
  nesterov = FALSE
)
}
\description{
Implements stochastic gradient descent (optionally with momentum).
Nesterov momentum is based on the formula from
On the importance of initialization and momentum in deep learning.
}
\section{Fields and Methods}{

See \code{\link{OptimizerIgnite}}.
}

\examples{
if (torch_is_installed()) {
\dontrun{
optimizer <- optim_ignite_sgd(model$parameters(), lr = 0.1)
optimizer$zero_grad()
loss_fn(model(input), target)$backward()
optimizer$step()
}
}
}
