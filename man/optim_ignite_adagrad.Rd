% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/ignite.R
\name{optim_ignite_adagrad}
\alias{optim_ignite_adagrad}
\title{LibTorch implementation of Adagrad}
\usage{
optim_ignite_adagrad(
  params,
  lr = 0.001,
  momentum = 0.9,
  dampening = 0,
  weight_decay = 0.01,
  nesterov = FALSE
)
}
\description{
Proposed in \href{https://jmlr.org/papers/v12/duchi11a.html}{Adaptive Subgradient Methods for Online Learning and Stochastic Optimization}
}
\section{Methods}{

TODO:
}

\section{Fields}{

}

\examples{
if (torch_is_installed()) {
\dontrun{
optimizer <- optim_ignite_adagrad(model$parameters(), lr = 0.1)
optimizer$zero_grad()
loss_fn(model(input), target)$backward()
optimizer$step()
}
}
}
